{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxW37g8cZ5PJKkhKsOm6yO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pravinkr05/Data-Mining-Project/blob/main/Classifier%20Algorithms/Bayesian_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The steps of Naive Bayes Classifier algorithms:\n",
        "\n",
        "1. **Data Prep**: Get your dataset ready.\n",
        "\n",
        "2. **Class Probabilities**:\n",
        "   - Calculate how often each class occurs:\n",
        "     - P(C) = (Number of instances with class C) / (Total number of instances)\n",
        "     \n",
        "\n",
        "3. **Feature Probabilities**:\n",
        "   - For each feature, find out how often it appears in each class:\n",
        "     - P(X_i | C) = (Number of instances with feature value X_i and class C) / (Number of instances with class C)\n",
        "\n",
        "4. **Predictions**:\n",
        "   - Use Bayes' theorem to calculate the probability of each class for a new data point:\n",
        "     - ( P(C | x) = (P(C) * P(x | C)) / P(x)\n",
        "     - Here,  P(x)  is constant for all classes, so we compare  P(C) * P(x | C)  directly.\n",
        "   - Pick the class with the highest probability as the predicted class for the new data point.\n",
        "\n",
        "5. **Evaluation**: Test the model with some data you've held back and see how well it does.\n"
      ],
      "metadata": {
        "id": "Z9lQszPCvkYp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's consider a example where we have a dataset of weather conditions (sunny, rainy) and corresponding labels indicating whether people went for a picnic (yes, no).\n",
        "\n",
        "| Weather   | Picnic |\n",
        "|-----------|--------|\n",
        "| Sunny     | Yes    |\n",
        "| Rainy     | No     |\n",
        "| Sunny     | Yes    |\n",
        "| Sunny     | No     |\n",
        "| Rainy     | Yes    |\n",
        "\n",
        "Now, let's say we want to predict whether people will go for a picnic given the weather condition \"Sunny\".\n",
        "\n",
        "Here are the steps:\n",
        "\n",
        "1. **Calculate Class Probabilities**: Calculate the prior probability of each class (Picnic = Yes, Picnic = No).\n",
        "   - P(Picnic = Yes) = 3/5\n",
        "   - P(Picnic = No) = 2/5\n",
        "\n",
        "2. **Calculate Conditional Probabilities**: For each feature (Weather), calculate the conditional probability of each value (Sunny, Rainy) given each class (Picnic = Yes, Picnic = No).\n",
        "   - P(Weather = Sunny | Picnic = Yes) = 2/3\n",
        "   - P(Weather = Sunny | Picnic = No) = 1/2\n",
        "   - P(Weather = Rainy | Picnic = Yes) = 1/3\n",
        "   - P(Weather = Rainy | Picnic = No) = 1/2\n",
        "\n",
        "3. **Make Predictions**: For the new data point (Weather = Sunny), calculate the posterior probability of each class given the weather condition using Bayes' theorem and the conditional probabilities calculated earlier.\n",
        "   - P(Picnic = Yes | Weather = Sunny) ∝ P(Picnic = Yes) * P(Weather = Sunny | Picnic = Yes)\n",
        "   - P(Picnic = No | Weather = Sunny) ∝ P(Picnic = No) * P(Weather = Sunny | Picnic = No)\n",
        "\n",
        "   Normalize the probabilities to make them sum to 1, and predict the class with the highest posterior probability.\n",
        "\n",
        "4. **Evaluate Model**: Test the model on a separate testing dataset and evaluate its performance using metrics like accuracy, precision, recall, or F1-score.\n"
      ],
      "metadata": {
        "id": "P5i8so4KyR7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gaussian Naive Bayes Classifier for Discrete Classes:"
      ],
      "metadata": {
        "id": "Iikpu4F28aiS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Srd6fFOn8QGr",
        "outputId": "e58dff9e-8bfa-4a8c-d1c5-53676417f7f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Prepare the Dataset\n",
        "# Let's generate a simple dataset for classification\n",
        "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [1, 5], [2, 4], [3, 3], [4, 2], [5, 1]])\n",
        "y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])  # Binary classes: 0 and 1\n",
        "\n",
        "# Step 2: Implement the Gaussian Naive Bayes Classifier\n",
        "class GaussianNaiveBayesClassifier:\n",
        "    def __init__(self):\n",
        "        self.class_probs = None\n",
        "        self.mean = None\n",
        "        self.std = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Calculate prior probabilities of each class\n",
        "        self.class_probs = np.bincount(y) / len(y)\n",
        "\n",
        "        # Calculate mean and standard deviation for each feature in each class\n",
        "        self.mean = np.array([X[y == c].mean(axis=0) for c in np.unique(y)])\n",
        "        self.std = np.array([X[y == c].std(axis=0) for c in np.unique(y)])\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Calculate likelihood for each class\n",
        "        likelihood = np.exp(-0.5 * ((X[:, None] - self.mean) / self.std) ** 2) / (np.sqrt(2 * np.pi) * self.std)\n",
        "\n",
        "        # Calculate posterior probabilities using Bayes' theorem\n",
        "        posterior = likelihood.prod(axis=2) * self.class_probs\n",
        "\n",
        "        # Make predictions based on maximum posterior probability\n",
        "        y_pred = np.argmax(posterior, axis=1)\n",
        "        return y_pred\n",
        "\n",
        "# Step 3: Train the Model\n",
        "# Instantiate the Gaussian Naive Bayes classifier\n",
        "gnb_cls = GaussianNaiveBayesClassifier()\n",
        "\n",
        "# Train the classifier\n",
        "gnb_cls.fit(X, y)\n",
        "\n",
        "# Step 4: Test the Model\n",
        "# Make predictions on the testing set (same as training set for simplicity)\n",
        "y_pred = gnb_cls.predict(X)\n",
        "\n",
        "# Step 5: Evaluate the Model\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gaussian Naive Bayes Regressor for Continuous Values:"
      ],
      "metadata": {
        "id": "7ceeaHa_8v8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Prepare the Dataset\n",
        "# Let's generate a simple dataset for regression\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 1)  # Feature\n",
        "y = 2 * X.squeeze() + np.random.randn(100)  # Continuous target\n",
        "\n",
        "# Step 2: Implement the Gaussian Naive Bayes Regressor\n",
        "class GaussianNaiveBayesRegressor:\n",
        "    def __init__(self):\n",
        "        self.mean = None\n",
        "        self.std = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Calculate mean and standard deviation of the target variable\n",
        "        self.mean = np.mean(y)\n",
        "        self.std = np.std(y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Calculate likelihood for each data point using Gaussian distribution\n",
        "        likelihood = (1 / (np.sqrt(2 * np.pi) * self.std)) * np.exp(-(X - self.mean) ** 2 / (2 * self.std ** 2))\n",
        "\n",
        "        # No prior probabilities involved in regression\n",
        "\n",
        "        # Posterior probability is proportional to likelihood in regression\n",
        "\n",
        "        # Make predictions based on likelihood\n",
        "        y_pred = likelihood\n",
        "        return y_pred\n",
        "\n",
        "# Step 3: Train the Model\n",
        "# Instantiate the Gaussian Naive Bayes regressor\n",
        "gnb_reg = GaussianNaiveBayesRegressor()\n",
        "\n",
        "# Train the regressor\n",
        "gnb_reg.fit(X, y)\n",
        "\n",
        "# Step 4: Test the Model\n",
        "# Make predictions on the testing set (same as training set for simplicity)\n",
        "y_pred = gnb_reg.predict(X)\n",
        "\n",
        "# Step 5: Evaluate the Model\n",
        "# No evaluation needed for regression in this simple example\n"
      ],
      "metadata": {
        "id": "YyIJgHFj8uah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gaussian Naive Bayes Classifier and regression using scikit-learn:"
      ],
      "metadata": {
        "id": "szUTmnnA9Fhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Prepare the Dataset\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split the Dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train the Model\n",
        "# Instantiate the Gaussian Naive Bayes classifier\n",
        "gnb_cls = GaussianNB()\n",
        "\n",
        "# Train the classifier\n",
        "gnb_cls.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Test the Model\n",
        "# Make predictions on the testing set\n",
        "y_pred = gnb_cls.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate the Model\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6lY81tu9C7U",
        "outputId": "5e5761cd-4e92-4458-87ef-e6735060d01b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    }
  ]
}